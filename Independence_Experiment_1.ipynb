{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPkZyDwF7rLFOn1odIbCVBH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aditi-Yadav-19/ColabTest/blob/main/Independence_Experiment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Mkc5WOjbL8J",
        "outputId": "cf488bdf-ebd5-4520-c065-2f5dceee1756"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.2 tokenizers-0.13.3 transformers-4.31.0\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.14.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.16.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.14.4 dill-0.3.7 multiprocess-0.70.15 xxhash-3.3.0\n",
            "Collecting peft\n",
            "  Downloading peft-0.4.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.0.1+cu118)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.31.0)\n",
            "Collecting accelerate (from peft)\n",
            "  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.3.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->peft) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->peft) (16.0.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.16.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers->peft) (2023.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
            "Installing collected packages: accelerate, peft\n",
            "Successfully installed accelerate-0.21.0 peft-0.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install peft"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers\n",
        "!pip install sentencepiece\n",
        "# from transformers import HfApi\n",
        "\n",
        "# Authenticate your session\n",
        "# api = HfApi()\n",
        "# api.login(token=\"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\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adLT4FCXfE66",
        "outputId": "505ee297-38c1-47e1-e3ca-fdaff50d83a9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import logging\n",
        "import re\n",
        "import numpy as np\n",
        "import torch\n",
        "import argparse\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    Trainer,\n",
        "    Pipeline,\n",
        "\n",
        ")\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "from peft import prepare_model_for_int8_training, LoraConfig, get_peft_model\n",
        "\n",
        "\n",
        "# Replace 'YOUR_HF_API_TOKEN' with your actual Hugging Face API token\n",
        "# os.environ[\"TRANSFORMERS_OFFLINE\"] = \"0\"\n",
        "# os.environ[\"HF_HOME\"] = \"/content/.cache/huggingface\"\n",
        "# os.environ[\"HF_HOME_TOKEN\"] = \"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\"\n",
        "\n",
        "\n",
        "\n",
        "# Define the main() function as a cell in Jupyter Notebook\n",
        "def main():\n",
        "    sample_size = 10000  # Set your desired sample size here\n",
        "    model_name = \"openlm-research/open_llama_3b\"\n",
        "    batch_size = 128\n",
        "    output_dir = \"AlpacaWeights\"\n",
        "\n",
        "    # Load tokenizer and model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\", )\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
        "\n",
        "    # Load dataset\n",
        "    data = load_dataset(\"gbharti/finance-alpaca\")\n",
        "\n",
        "    # Create a sample of 1000 items from the 'train' split\n",
        "    sampled_data = data[\"train\"].shuffle(seed=42).select(range(sample_size))\n",
        "    sampled_dataset_dict = DatasetDict({\"train\": sampled_data})\n",
        "    data = sampled_dataset_dict\n",
        "\n",
        "    # Generate prompt for each data point\n",
        "    def generate_prompt(data_point):\n",
        "        # taken from https://github.com/tloen/alpaca-lora\n",
        "        if data_point[\"instruction\"]:\n",
        "            return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "    ### Instruction:\n",
        "    {data_point[\"instruction\"]}\n",
        "\n",
        "    ### Input:\n",
        "    {data_point[\"input\"]}\n",
        "\n",
        "    ### Response:\n",
        "    {data_point[\"output\"]}\"\"\"\n",
        "        else:\n",
        "            return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "    ### Instruction:\n",
        "    {data_point[\"instruction\"]}\n",
        "\n",
        "    ### Response:\n",
        "    {data_point[\"output\"]}\"\"\"\n",
        "\n",
        "    data = data.map(\n",
        "        lambda data_point: {\"prompt\": tokenizer(generate_prompt(data_point))}\n",
        "    )\n",
        "\n",
        "    # Settings for A100 - For 3090\n",
        "    MICRO_BATCH_SIZE = 4  # change to 4 for 3090\n",
        "    BATCH_SIZE = 16\n",
        "    GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n",
        "    EPOCHS = 2  # paper uses 3\n",
        "    LEARNING_RATE = 2e-5\n",
        "    CUTOFF_LEN = 256\n",
        "    LORA_R = 4\n",
        "    LORA_ALPHA = 16\n",
        "    LORA_DROPOUT = 0.05\n",
        "\n",
        "    model = prepare_model_for_int8_training(model, use_gradient_checkpointing=True)\n",
        "\n",
        "    config = LoraConfig(\n",
        "        r=LORA_R,\n",
        "        lora_alpha=LORA_ALPHA,\n",
        "        lora_dropout=LORA_DROPOUT,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "    model = get_peft_model(model, config)\n",
        "    tokenizer.pad_token_id = 0  # unk. we want this to be different from the eos token\n",
        "\n",
        "    data = data.shuffle().map(\n",
        "        lambda data_point: tokenizer(\n",
        "            generate_prompt(data_point),\n",
        "            truncation=True,\n",
        "            max_length=CUTOFF_LEN,\n",
        "            padding=\"max_length\",\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Trainer initialization\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        train_dataset=data[\"train\"],\n",
        "        args=TrainingArguments(\n",
        "            per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
        "            gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "            warmup_steps=100,\n",
        "            num_train_epochs=EPOCHS,\n",
        "            learning_rate=LEARNING_RATE,\n",
        "            logging_steps=100,\n",
        "            output_dir=\"lora-weights\",\n",
        "            save_total_limit=3,\n",
        "        ),\n",
        "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        "    )\n",
        "    model.config.use_cache = False\n",
        "    trainer.train(resume_from_checkpoint=False)\n",
        "\n",
        "    model.save_pretrained(\"lora-weights\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "q53O-RYVb7lg",
        "outputId": "140c4ddf-c025-40bc-a570-aa9a88f6ac72"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-816a8989ecc9>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m from transformers import (\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import transformers\n",
        "# import torch\n",
        "# from peft import PeftModel\n",
        "\n",
        "# def model_fn(model_dir):\n",
        "#     model = transformers.AutoModelForCausalLM.from_pretrained(model_dir)\n",
        "#     adapters_name = \"lora-weights\"\n",
        "#     model = PeftModel.from_pretrained(model, adapters_name)\n",
        "#     model.eval()\n",
        "\n",
        "#     tokenizer = transformers.AutoTokenizer.from_pretrained(model_dir)\n",
        "\n",
        "#     return model, tokenizer\n",
        "\n",
        "# def predict_fn(data, model_and_tokenizer):\n",
        "#     model, tokenizer = model_and_tokenizer\n",
        "\n",
        "#     batch = tokenizer(data, return_tensors=\"pt\", add_special_tokens=False)\n",
        "\n",
        "#     input_ids = batch[\"input_ids\"].to(model.device)\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         generated = model.generate(\n",
        "#             input_ids=input_ids,\n",
        "#             min_length=10,\n",
        "#             max_length=60,\n",
        "#             do_sample=True,\n",
        "#             top_k=50,\n",
        "#             temperature=1.0,\n",
        "#             top_p=1.0,\n",
        "#             repetition_penalty=1.1,\n",
        "#         )\n",
        "#         response = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
        "\n",
        "#     return response\n",
        "\n",
        "# # Set your model directory and input sentence here\n",
        "# model_dir = \"Maykeye/TinyLLama-v0\"\n",
        "# input_sentence = \"Joe Biden is\"\n",
        "\n",
        "# model_and_tokenizer = model_fn(model_dir)\n",
        "# prediction = predict_fn(input_sentence, model_and_tokenizer)\n",
        "\n",
        "# print(\"Generated text:\", prediction)\n",
        "\n",
        "\n",
        "\n",
        "# import transformers\n",
        "# import torch\n",
        "# from peft import PeftModel\n",
        "\n",
        "# def model_fn(model_identifier):\n",
        "#     model = transformers.AutoModelForCausalLM.from_pretrained(model_identifier)\n",
        "#     adapters_name = \"lora-weights\"\n",
        "#     model = PeftModel.from_pretrained(model, adapters_name)\n",
        "#     model.eval()\n",
        "\n",
        "#     tokenizer = transformers.AutoTokenizer.from_pretrained(model_identifier)\n",
        "\n",
        "#     return model, tokenizer\n",
        "\n",
        "# def predict_fn(question, model_and_tokenizer):\n",
        "#     model, tokenizer = model_and_tokenizer\n",
        "\n",
        "#     prompt = f\"Question: {question}\\nAnswer:\"\n",
        "\n",
        "#     input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         generated = model.generate(\n",
        "#             input_ids=input_ids,\n",
        "#             max_length=100,  # Adjust the max length as needed\n",
        "#             do_sample=True,\n",
        "#             top_k=50,\n",
        "#             temperature=1.0,\n",
        "#             top_p=1.0,\n",
        "#             repetition_penalty=1.1,\n",
        "#         )\n",
        "#         response = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
        "\n",
        "#     return response\n",
        "\n",
        "# # Set your model identifier and input question here\n",
        "# model_identifier = \"Maykeye/TinyLLama-v0\"\n",
        "# input_question = \"What is the capital of France?\"\n",
        "\n",
        "# # Load model and tokenizer\n",
        "# model_and_tokenizer = model_fn(model_identifier)\n",
        "\n",
        "# # Generate answer to the question\n",
        "# answer = predict_fn(input_question, model_and_tokenizer)\n",
        "\n",
        "# # Print the generated answer\n",
        "# print(\"Generated answer:\", answer)\n",
        "\n",
        "\n",
        "import transformers\n",
        "import torch\n",
        "from peft import PeftModel\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Define your fine-tuned finance model directory\n",
        "finance_model_identifier = \"Maykeye/TinyLLama-v0\"\n",
        "\n",
        "def model_fn(model_identifier):\n",
        "    model = transformers.AutoModelForCausalLM.from_pretrained(model_identifier)\n",
        "    adapters_name = \"lora-weights\"\n",
        "    model = PeftModel.from_pretrained(model, adapters_name)\n",
        "    model.eval()\n",
        "\n",
        "    tokenizer = transformers.AutoTokenizer.from_pretrained(model_identifier)\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "def predict_fn(question, model_and_tokenizer):\n",
        "    model, tokenizer = model_and_tokenizer\n",
        "\n",
        "    prompt = f\"Finance Question: {question}\\nAnswer:\"\n",
        "\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            max_length=100,  # Adjust the max length as needed\n",
        "            do_sample=True,\n",
        "            top_k=50,\n",
        "            temperature=1.0,\n",
        "            top_p=1.0,\n",
        "            repetition_penalty=1.1,\n",
        "        )\n",
        "        response = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
        "\n",
        "    return response\n",
        "\n",
        "# Set your finance model identifier and input finance question here\n",
        "finance_model_and_tokenizer = model_fn(finance_model_identifier)\n",
        "input_finance_question = \"What is a stock market?\"\n",
        "\n",
        "# Generate answer to the finance question\n",
        "finance_answer = predict_fn(input_finance_question, finance_model_and_tokenizer)\n",
        "\n",
        "# Print the generated finance answer\n",
        "print(\"Generated finance answer:\", finance_answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FfinHebi71z",
        "outputId": "81b95a6d-baba-41a5-a670-424f5fa3c7ce"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated finance answer: Finance Question: What is a stock market?\n",
            "Answer: I can learn and be nice. Would you are so ugly for her to give my courage to have someone else for him from before yes, why you's always be excited to keep your advice. Can you be a professional time with the same adventure? 7 says that you might do something very adventurous. We can find out what we want to buy by when we are doing stories.\n",
            "Ben has a special memory and\n"
          ]
        }
      ]
    }
  ]
}