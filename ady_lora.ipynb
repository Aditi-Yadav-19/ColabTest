{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMzkzbnk8gGM1uLLkza3NvH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "87302a3614e74addbcad9f84c10a5179": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a46515bfca634ae8be232e578adf4a4e",
              "IPY_MODEL_99b6ca159ece446c8a3f4eb0912fc79a",
              "IPY_MODEL_ed5376435dfc44ea96f595c95f433b53"
            ],
            "layout": "IPY_MODEL_2bb2ca8e0f004d7a8029c90644e2da35"
          }
        },
        "a46515bfca634ae8be232e578adf4a4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ec22a2d4f98462cbf529f040f772141",
            "placeholder": "​",
            "style": "IPY_MODEL_4b7112ed0b4c4e52bec2a8a7c547b464",
            "value": "Map: 100%"
          }
        },
        "99b6ca159ece446c8a3f4eb0912fc79a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_595155182ce3434196aea5764c93c8fc",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_08d0029ec1ca4757a941e8dd43efeaff",
            "value": 100
          }
        },
        "ed5376435dfc44ea96f595c95f433b53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33c1aa55c02e426192d8b6acd270db19",
            "placeholder": "​",
            "style": "IPY_MODEL_237c141e327f4b2898a7f55be9333e89",
            "value": " 100/100 [00:00&lt;00:00, 431.28 examples/s]"
          }
        },
        "2bb2ca8e0f004d7a8029c90644e2da35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ec22a2d4f98462cbf529f040f772141": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b7112ed0b4c4e52bec2a8a7c547b464": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "595155182ce3434196aea5764c93c8fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08d0029ec1ca4757a941e8dd43efeaff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "33c1aa55c02e426192d8b6acd270db19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "237c141e327f4b2898a7f55be9333e89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aditi-Yadav-19/ColabTest/blob/main/ady_lora.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "D4g2r_LMuQFK",
        "outputId": "20f4b2f9-3319-4525-99a5-6481789d48f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: ipykernel_launcher.py [-h] [--sample_size SAMPLE_SIZE]\n",
            "                             [--model_name MODEL_NAME]\n",
            "                             [--batch_size BATCH_SIZE]\n",
            "                             [--output_dir OUTPUT_DIR]\n",
            "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-f7660b2d-fde1-4980-b1f0-ce96661fe528.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import logging\n",
        "import re\n",
        "import numpy as np\n",
        "import torch\n",
        "import argparse\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    Trainer,\n",
        "    Pipeline,\n",
        ")\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "from peft import prepare_model_for_int8_training, LoraConfig, get_peft_model\n",
        "\n",
        "# Set up logging\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Constants for keys and format\n",
        "INSTRUCTION_KEY = \"### Instruction:\"\n",
        "RESPONSE_KEY = \"### Response:\"\n",
        "END_KEY = \"### End\"\n",
        "INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
        "PROMPT_FOR_GENERATION_FORMAT = \"\"\"{intro}\n",
        "{instruction_key}\n",
        "{instruction}\n",
        "{response_key}\n",
        "\"\"\".format(\n",
        "    intro=INTRO_BLURB,\n",
        "    instruction_key=INSTRUCTION_KEY,\n",
        "    instruction=\"{instruction}\",\n",
        "    response_key=RESPONSE_KEY,\n",
        ")\n",
        "\n",
        "\n",
        "class InstructionTextGenerationPipeline(Pipeline):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *args,\n",
        "        do_sample: bool = True,\n",
        "        max_new_tokens: int = 256,\n",
        "        top_p: float = 0.92,\n",
        "        top_k: int = 0,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(\n",
        "            *args,\n",
        "            do_sample=do_sample,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            top_p=top_p,\n",
        "            top_k=top_k,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "    def _sanitize_parameters(self, return_instruction_text=False, **generate_kwargs):\n",
        "        preprocess_params = {}\n",
        "\n",
        "        # newer versions of the tokenizer configure the response key as a special token.  newer versions still may\n",
        "        # append a newline to yield a single token.  find whatever token is configured for the response key.\n",
        "        tokenizer_response_key = next(\n",
        "            (\n",
        "                token\n",
        "                for token in self.tokenizer.additional_special_tokens\n",
        "                if token.startswith(RESPONSE_KEY)\n",
        "            ),\n",
        "            None,\n",
        "        )\n",
        "\n",
        "        response_key_token_id = None\n",
        "        end_key_token_id = None\n",
        "        if tokenizer_response_key:\n",
        "            try:\n",
        "                response_key_token_id = get_special_token_id(\n",
        "                    self.tokenizer, tokenizer_response_key\n",
        "                )\n",
        "                end_key_token_id = get_special_token_id(self.tokenizer, END_KEY)\n",
        "\n",
        "                # Ensure generation stops once it generates \"### End\"\n",
        "                generate_kwargs[\"eos_token_id\"] = end_key_token_id\n",
        "            except ValueError:\n",
        "                pass\n",
        "\n",
        "        forward_params = generate_kwargs\n",
        "        postprocess_params = {\n",
        "            \"response_key_token_id\": response_key_token_id,\n",
        "            \"end_key_token_id\": end_key_token_id,\n",
        "            \"return_instruction_text\": return_instruction_text,\n",
        "        }\n",
        "\n",
        "        return preprocess_params, forward_params, postprocess_params\n",
        "\n",
        "    def preprocess(self, instruction_text, **generate_kwargs):\n",
        "        prompt_text = PROMPT_FOR_GENERATION_FORMAT.format(instruction=instruction_text)\n",
        "        inputs = self.tokenizer(\n",
        "            prompt_text,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        inputs[\"prompt_text\"] = prompt_text\n",
        "        inputs[\"instruction_text\"] = instruction_text\n",
        "        return inputs\n",
        "\n",
        "    def _forward(self, model_inputs, **generate_kwargs):\n",
        "        input_ids = model_inputs[\"input_ids\"]\n",
        "        attention_mask = model_inputs.get(\"attention_mask\", None)\n",
        "        generated_sequence = self.model.generate(\n",
        "            input_ids=input_ids.to(self.model.device),\n",
        "            attention_mask=attention_mask,\n",
        "            pad_token_id=self.tokenizer.pad_token_id,\n",
        "            **generate_kwargs,\n",
        "        )[0].cpu()\n",
        "        instruction_text = model_inputs.pop(\"instruction_text\")\n",
        "        return {\n",
        "            \"generated_sequence\": generated_sequence,\n",
        "            \"input_ids\": input_ids,\n",
        "            \"instruction_text\": instruction_text,\n",
        "        }\n",
        "\n",
        "    def postprocess(\n",
        "        self,\n",
        "        model_outputs,\n",
        "        response_key_token_id,\n",
        "        end_key_token_id,\n",
        "        return_instruction_text,\n",
        "    ):\n",
        "        sequence = model_outputs[\"generated_sequence\"]\n",
        "        instruction_text = model_outputs[\"instruction_text\"]\n",
        "\n",
        "        # The response will be set to this variable if we can identify it.\n",
        "        decoded = None\n",
        "\n",
        "        # If we have token IDs for the response and end, then we can find the tokens and only decode between them.\n",
        "        if response_key_token_id and end_key_token_id:\n",
        "            # Find where \"### Response:\" is first found in the generated tokens.  Considering this is part of the\n",
        "            # prompt, we should definitely find it.  We will return the tokens found after this token.\n",
        "            response_pos = None\n",
        "            response_positions = np.where(sequence == response_key_token_id)[0]\n",
        "            if len(response_positions) == 0:\n",
        "                logger.warn(\n",
        "                    f\"Could not find response key {response_key_token_id} in: {sequence}\"\n",
        "                )\n",
        "            else:\n",
        "                response_pos = response_positions[0]\n",
        "\n",
        "            if response_pos:\n",
        "                # Next find where \"### End\" is located.  The model has been trained to end its responses with this\n",
        "                # sequence (or actually, the token ID it maps to, since it is a special token).  We may not find\n",
        "                # this token, as the response could be truncated.  If we don't find it then just return everything\n",
        "                # to the end.  Note that even though we set eos_token_id, we still see the this token at the end.\n",
        "                end_pos = None\n",
        "                end_positions = np.where(sequence == end_key_token_id)[0]\n",
        "                if len(end_positions) > 0:\n",
        "                    end_pos = end_positions[0]\n",
        "\n",
        "                decoded = self.tokenizer.decode(\n",
        "                    sequence[response_pos + 1 : end_pos]\n",
        "                ).strip()\n",
        "        else:\n",
        "            # Otherwise we'll decode everything and use a regex to find the response and end.\n",
        "\n",
        "            fully_decoded = self.tokenizer.decode(sequence)\n",
        "\n",
        "            # The response appears after \"### Response:\".  The model has been trained to append \"### End\" at the\n",
        "            # end.\n",
        "            m = re.search(\n",
        "                r\"#+\\s*Response:\\s*(.+?)#+\\s*End\", fully_decoded, flags=re.DOTALL\n",
        "            )\n",
        "\n",
        "            if m:\n",
        "                decoded = m.group(1).strip()\n",
        "            else:\n",
        "                # The model might not generate the \"### End\" sequence before reaching the max tokens.  In this case,\n",
        "                # return everything after \"### Response:\".\n",
        "                m = re.search(r\"#+\\s*Response:\\s*(.+)\", fully_decoded, flags=re.DOTALL)\n",
        "                if m:\n",
        "                    decoded = m.group(1).strip()\n",
        "                else:\n",
        "                    logger.warn(f\"Failed to find response in:\\n{fully_decoded}\")\n",
        "\n",
        "        if return_instruction_text:\n",
        "            return {\"instruction_text\": instruction_text, \"generated_text\": decoded}\n",
        "\n",
        "        return decoded\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run ALPaCA LORA training script.\"\"\"\n",
        "    parser = argparse.ArgumentParser(description=\"Run ALPaCA LORA training script\")\n",
        "    parser.add_argument(\n",
        "        \"--sample_size\", type=int, default=100, help=\"Number of samples\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--model_name\",\n",
        "        type=str,\n",
        "        default=\"Maykeye/TinyLLama-v0\",\n",
        "        help=\"Pretrained model name\",\n",
        "    )\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=128, help=\"Batch size\")\n",
        "    parser.add_argument(\n",
        "        \"--output_dir\", type=str, default=\"AlpacaWeights\", help=\"Output Directory\"\n",
        "    )\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Load tokenizer and model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.model_name, padding_side=\"left\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(args.model_name, device_map=\"auto\")\n",
        "\n",
        "    # Load dataset\n",
        "    data = load_dataset(\"tatsu-lab/alpaca\")\n",
        "\n",
        "    # Create a sample of 1000 items from the 'train' split\n",
        "    sampled_data = data[\"train\"].shuffle(seed=42).select(range(args.sample_size))\n",
        "\n",
        "    # Create a new DatasetDict with the updated 'train' split\n",
        "    sampled_dataset_dict = DatasetDict({\"train\": sampled_data})\n",
        "\n",
        "    data = sampled_dataset_dict\n",
        "\n",
        "    # Generate prompt for each data point\n",
        "    def generate_prompt(data_point):\n",
        "        # taken from https://github.com/tloen/alpaca-lora\n",
        "        if data_point[\"instruction\"]:\n",
        "            return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "    ### Instruction:\n",
        "    {data_point[\"instruction\"]}\n",
        "\n",
        "    ### Input:\n",
        "    {data_point[\"input\"]}\n",
        "\n",
        "    ### Response:\n",
        "    {data_point[\"output\"]}\"\"\"\n",
        "        else:\n",
        "            return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "    ### Instruction:\n",
        "    {data_point[\"instruction\"]}\n",
        "\n",
        "    ### Response:\n",
        "    {data_point[\"output\"]}\"\"\"\n",
        "\n",
        "    data = data.map(\n",
        "        lambda data_point: {\"prompt\": tokenizer(generate_prompt(data_point))}\n",
        "    )\n",
        "\n",
        "    # Settings for A100 - For 3090\n",
        "    MICRO_BATCH_SIZE = 4  # change to 4 for 3090\n",
        "    BATCH_SIZE = args.batch_size\n",
        "    GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n",
        "    EPOCHS = 2  # paper uses 3\n",
        "    LEARNING_RATE = 2e-5\n",
        "    CUTOFF_LEN = 256\n",
        "    LORA_R = 4\n",
        "    LORA_ALPHA = 16\n",
        "    LORA_DROPOUT = 0.05\n",
        "\n",
        "    model = prepare_model_for_int8_training(model, use_gradient_checkpointing=True)\n",
        "\n",
        "    config = LoraConfig(\n",
        "        r=LORA_R,\n",
        "        lora_alpha=LORA_ALPHA,\n",
        "        lora_dropout=LORA_DROPOUT,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "    model = get_peft_model(model, config)\n",
        "    tokenizer.pad_token_id = 0  # unk. we want this to be different from the eos token\n",
        "\n",
        "    data = data.shuffle().map(\n",
        "        lambda data_point: tokenizer(\n",
        "            generate_prompt(data_point),\n",
        "            truncation=True,\n",
        "            max_length=CUTOFF_LEN,\n",
        "            padding=\"max_length\",\n",
        "        )\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        train_dataset=data[\"train\"],\n",
        "        args=TrainingArguments(\n",
        "            per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
        "            gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "            warmup_steps=100,\n",
        "            num_train_epochs=EPOCHS,\n",
        "            learning_rate=LEARNING_RATE,\n",
        "            logging_steps=100,\n",
        "            output_dir=\"lora-weights\",\n",
        "            save_total_limit=3,\n",
        "        ),\n",
        "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        "    )\n",
        "    model.config.use_cache = False\n",
        "    trainer.train(resume_from_checkpoint=False)\n",
        "\n",
        "    model.save_pretrained(\"lora-weights\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "!pip install accelerate\n",
        "!pip install appdirs\n",
        "!pip install loralib\n",
        "!pip install bitsandbytes\n",
        "!pip install black\n",
        "!pip install black[jupyter]\n",
        "!pip install datasets\n",
        "!pip install fire\n",
        "!pip install git+https://github.com/huggingface/peft.git\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install sentencepiece\n",
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFEi28TIuWYj",
        "outputId": "41279850-9f66-4c88-e0d9-0646717ce4c4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.21.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (1.4.4)\n",
            "Requirement already satisfied: loralib in /usr/local/lib/python3.10/dist-packages (0.1.1)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.41.1)\n",
            "Requirement already satisfied: black in /usr/local/lib/python3.10/dist-packages (23.7.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black) (8.1.6)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from black) (1.0.0)\n",
            "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.10/dist-packages (from black) (23.1)\n",
            "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from black) (0.11.2)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black) (3.10.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black) (2.0.1)\n",
            "Requirement already satisfied: black[jupyter] in /usr/local/lib/python3.10/dist-packages (23.7.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black[jupyter]) (8.1.6)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from black[jupyter]) (1.0.0)\n",
            "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.10/dist-packages (from black[jupyter]) (23.1)\n",
            "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from black[jupyter]) (0.11.2)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black[jupyter]) (3.10.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black[jupyter]) (2.0.1)\n",
            "Requirement already satisfied: ipython>=7.8.0 in /usr/local/lib/python3.10/dist-packages (from black[jupyter]) (7.34.0)\n",
            "Requirement already satisfied: tokenize-rt>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from black[jupyter]) (5.2.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.8.0->black[jupyter]) (67.7.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.8.0->black[jupyter]) (0.19.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=7.8.0->black[jupyter]) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=7.8.0->black[jupyter]) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.8.0->black[jupyter]) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.8.0->black[jupyter]) (3.0.39)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=7.8.0->black[jupyter]) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=7.8.0->black[jupyter]) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=7.8.0->black[jupyter]) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.8.0->black[jupyter]) (4.8.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=7.8.0->black[jupyter]) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=7.8.0->black[jupyter]) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.8.0->black[jupyter]) (0.2.6)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.3.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.16.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.10/dist-packages (0.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire) (2.3.0)\n",
            "Collecting git+https://github.com/huggingface/peft.git\n",
            "  Cloning https://github.com/huggingface/peft.git to /tmp/pip-req-build-nq3p438k\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git /tmp/pip-req-build-nq3p438k\n",
            "  Resolved https://github.com/huggingface/peft.git to commit 369a0fba85a94e99738f832877c665a25793da1f\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft==0.5.0.dev0) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.5.0.dev0) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft==0.5.0.dev0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft==0.5.0.dev0) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.5.0.dev0) (2.0.1+cu118)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft==0.5.0.dev0) (4.32.0.dev0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft==0.5.0.dev0) (4.66.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from peft==0.5.0.dev0) (0.21.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft==0.5.0.dev0) (0.3.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.5.0.dev0) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.5.0.dev0) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.5.0.dev0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.5.0.dev0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.5.0.dev0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.5.0.dev0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->peft==0.5.0.dev0) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->peft==0.5.0.dev0) (16.0.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.5.0.dev0) (0.16.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.5.0.dev0) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.5.0.dev0) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.5.0.dev0) (0.13.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers->peft==0.5.0.dev0) (2023.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft==0.5.0.dev0) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.5.0.dev0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.5.0.dev0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.5.0.dev0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.5.0.dev0) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft==0.5.0.dev0) (1.3.0)\n",
            "Collecting git+https://github.com/huggingface/transformers.git\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-nlbmrxex\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-nlbmrxex\n",
            "  Resolved https://github.com/huggingface/transformers.git to commit 3f9cb335047315edfd4b6ad666ef148e98cc4850\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.32.0.dev0) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.32.0.dev0) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.32.0.dev0) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.32.0.dev0) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.32.0.dev0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.32.0.dev0) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.32.0.dev0) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.32.0.dev0) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.32.0.dev0) (0.3.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.32.0.dev0) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.32.0.dev0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.32.0.dev0) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.32.0.dev0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.32.0.dev0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.32.0.dev0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.32.0.dev0) (2023.7.22)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (3.40.1)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: aiohttp~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.8.5)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from gradio) (0.101.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.3.1)\n",
            "Requirement already satisfied: gradio-client>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.4.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from gradio) (0.24.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.16.4)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.2)\n",
            "Requirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.0)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.3)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: mdit-py-plugins<=0.3.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.3.3)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.23.5)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.9.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (23.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.1)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.6)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Requirement already satisfied: requests~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.31.0)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.7.1)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.23.2)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client>=0.4.0->gradio) (2023.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (3.12.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (4.66.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio) (0.1.2)\n",
            "Requirement already satisfied: linkify-it-py<3,>=1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio) (2.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.42.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->gradio) (0.5.0)\n",
            "Requirement already satisfied: pydantic-core==2.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->gradio) (2.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (2023.7.22)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (8.1.6)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.14.0)\n",
            "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (0.27.0)\n",
            "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (0.17.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.3.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx->gradio) (3.7.1)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.9.2)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.10/dist-packages (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio) (1.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->httpcore<0.18.0,>=0.15.0->httpx->gradio) (1.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import logging\n",
        "import re\n",
        "import numpy as np\n",
        "import torch\n",
        "import argparse\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    Trainer,\n",
        "    Pipeline,\n",
        ")\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "from peft import prepare_model_for_int8_training, LoraConfig, get_peft_model\n",
        "\n",
        "# Rest of the code remains the same until the main() function\n",
        "\n",
        "# Define the main() function as a cell in Jupyter Notebook\n",
        "def main():\n",
        "    sample_size = 100  # Set your desired sample size here\n",
        "    model_name = \"Maykeye/TinyLLama-v0\"\n",
        "    batch_size = 128\n",
        "    output_dir = \"AlpacaWeights\"\n",
        "\n",
        "    # Load tokenizer and model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
        "\n",
        "    # Load dataset\n",
        "    data = load_dataset(\"tatsu-lab/alpaca\")\n",
        "\n",
        "    # Create a sample of 1000 items from the 'train' split\n",
        "    sampled_data = data[\"train\"].shuffle(seed=42).select(range(sample_size))\n",
        "    sampled_dataset_dict = DatasetDict({\"train\": sampled_data})\n",
        "    data = sampled_dataset_dict\n",
        "\n",
        "    # Generate prompt for each data point\n",
        "    def generate_prompt(data_point):\n",
        "        # taken from https://github.com/tloen/alpaca-lora\n",
        "        if data_point[\"instruction\"]:\n",
        "            return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "    ### Instruction:\n",
        "    {data_point[\"instruction\"]}\n",
        "\n",
        "    ### Input:\n",
        "    {data_point[\"input\"]}\n",
        "\n",
        "    ### Response:\n",
        "    {data_point[\"output\"]}\"\"\"\n",
        "        else:\n",
        "            return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "    ### Instruction:\n",
        "    {data_point[\"instruction\"]}\n",
        "\n",
        "    ### Response:\n",
        "    {data_point[\"output\"]}\"\"\"\n",
        "\n",
        "    data = data.map(\n",
        "        lambda data_point: {\"prompt\": tokenizer(generate_prompt(data_point))}\n",
        "    )\n",
        "\n",
        "    # Settings for A100 - For 3090\n",
        "    MICRO_BATCH_SIZE = 4  # change to 4 for 3090\n",
        "    BATCH_SIZE = 16\n",
        "    GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n",
        "    EPOCHS = 2  # paper uses 3\n",
        "    LEARNING_RATE = 2e-5\n",
        "    CUTOFF_LEN = 256\n",
        "    LORA_R = 4\n",
        "    LORA_ALPHA = 16\n",
        "    LORA_DROPOUT = 0.05\n",
        "\n",
        "    model = prepare_model_for_int8_training(model, use_gradient_checkpointing=True)\n",
        "\n",
        "    config = LoraConfig(\n",
        "        r=LORA_R,\n",
        "        lora_alpha=LORA_ALPHA,\n",
        "        lora_dropout=LORA_DROPOUT,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "    model = get_peft_model(model, config)\n",
        "    tokenizer.pad_token_id = 0  # unk. we want this to be different from the eos token\n",
        "\n",
        "    data = data.shuffle().map(\n",
        "        lambda data_point: tokenizer(\n",
        "            generate_prompt(data_point),\n",
        "            truncation=True,\n",
        "            max_length=CUTOFF_LEN,\n",
        "            padding=\"max_length\",\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Trainer initialization\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        train_dataset=data[\"train\"],\n",
        "        args=TrainingArguments(\n",
        "            per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
        "            gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "            warmup_steps=100,\n",
        "            num_train_epochs=EPOCHS,\n",
        "            learning_rate=LEARNING_RATE,\n",
        "            logging_steps=100,\n",
        "            output_dir=\"lora-weights\",\n",
        "            save_total_limit=3,\n",
        "        ),\n",
        "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        "    )\n",
        "    model.config.use_cache = False\n",
        "    trainer.train(resume_from_checkpoint=False)\n",
        "\n",
        "    model.save_pretrained(\"lora-weights\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "87302a3614e74addbcad9f84c10a5179",
            "a46515bfca634ae8be232e578adf4a4e",
            "99b6ca159ece446c8a3f4eb0912fc79a",
            "ed5376435dfc44ea96f595c95f433b53",
            "2bb2ca8e0f004d7a8029c90644e2da35",
            "7ec22a2d4f98462cbf529f040f772141",
            "4b7112ed0b4c4e52bec2a8a7c547b464",
            "595155182ce3434196aea5764c93c8fc",
            "08d0029ec1ca4757a941e8dd43efeaff",
            "33c1aa55c02e426192d8b6acd270db19",
            "237c141e327f4b2898a7f55be9333e89"
          ]
        },
        "id": "_iM62AoDvTJs",
        "outputId": "44ae2033-b5d7-414f-8cf4-c1d1185f238a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "87302a3614e74addbcad9f84c10a5179"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [12/12 01:20, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import transformers\n",
        "import torch\n",
        "from peft import PeftModel\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fynbtCeKvegM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# def model_fn(model_dir):\n",
        "#     model = transformers.AutoModelForCausalLM.from_pretrained(model_dir)\n",
        "#     adapters_name = \"lora-weights\"\n",
        "#     model = PeftModel.from_pretrained(model, adapters_name)\n",
        "#     model.eval()\n",
        "\n",
        "#     tokenizer = transformers.AutoTokenizer.from_pretrained(model_dir)\n",
        "\n",
        "#     return model, tokenizer\n",
        "\n",
        "# def predict_fn(data, model_and_tokenizer):\n",
        "#     model, tokenizer = model_and_tokenizer\n",
        "\n",
        "#     batch = tokenizer(data, return_tensors=\"pt\", add_special_tokens=False)\n",
        "\n",
        "#     input_ids = batch[\"input_ids\"].to(model.device)\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         generated = model.generate(\n",
        "#             input_ids=input_ids,\n",
        "#             min_length=10,\n",
        "#             max_length=60,\n",
        "#             do_sample=True,\n",
        "#             top_k=50,\n",
        "#             temperature=1.0,\n",
        "#             top_p=1.0,\n",
        "#             repetition_penalty=1.1,\n",
        "#         )\n",
        "#         response = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
        "\n",
        "#     return response\n",
        "\n",
        "# # Set your model directory and input sentence here\n",
        "# model_dir = \"Maykeye/TinyLLama-v0\"\n",
        "# input_sentence = \"Joe Biden is\"\n",
        "\n",
        "# # Load model and tokenizer\n",
        "# model_and_tokenizer = model_fn(model_dir)\n",
        "\n",
        "# # Generate text\n",
        "# prediction = predict_fn(input_sentence, model_and_tokenizer)\n",
        "\n",
        "# # Print the generated text\n",
        "# print(\"Generated text:\", prediction)\n",
        "\n",
        "\n",
        "import transformers\n",
        "import torch\n",
        "from peft import PeftModel\n",
        "\n",
        "def model_fn(model_identifier):\n",
        "    model = transformers.AutoModelForCausalLM.from_pretrained(model_identifier)\n",
        "    adapters_name = \"lora-weights\"\n",
        "    model = PeftModel.from_pretrained(model, adapters_name)\n",
        "    model.eval()\n",
        "\n",
        "    tokenizer = transformers.AutoTokenizer.from_pretrained(model_identifier)\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "def predict_fn(question, model_and_tokenizer):\n",
        "    model, tokenizer = model_and_tokenizer\n",
        "\n",
        "    prompt = f\"Question: {question}\\nAnswer:\"\n",
        "\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            max_length=100,  # Adjust the max length as needed\n",
        "            do_sample=True,\n",
        "            top_k=50,\n",
        "            temperature=1.0,\n",
        "            top_p=1.0,\n",
        "            repetition_penalty=1.1,\n",
        "        )\n",
        "        response = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
        "\n",
        "    return response\n",
        "\n",
        "# Set your model identifier and input question here\n",
        "model_identifier = \"Maykeye/TinyLLama-v0\"\n",
        "input_question = \"What is the capital of France?\"\n",
        "\n",
        "# Load model and tokenizer\n",
        "model_and_tokenizer = model_fn(model_identifier)\n",
        "\n",
        "# Generate answer to the question\n",
        "answer = predict_fn(input_question, model_and_tokenizer)\n",
        "\n",
        "# Print the generated answer\n",
        "print(\"Generated answer:\", answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Lq3vaSIUoMb",
        "outputId": "7c40fcdc-74e8-469b-819f-cf4a309895fd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated answer: Question: What is the capital of France?\n",
            "Answer: \"Now you can fly!\"\n",
            "He walks through the park and then suddenly was all over with a big tree when he heard a bit of other animals. It was so bright!\n",
            "The lions were walking closer to them and laughed. \"Let's go up at the park.” \n",
            "They started sitting in the tree, just like a few times he noticed it looked very soft.\n",
            "The farmer said, \"What\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import torch\n",
        "from peft import PeftModel\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Define your fine-tuned finance model directory\n",
        "finance_model_identifier = \"Maykeye/TinyLLama-v0\"\n",
        "\n",
        "def model_fn(model_identifier):\n",
        "    model = transformers.AutoModelForCausalLM.from_pretrained(model_identifier)\n",
        "    adapters_name = \"lora-weights\"\n",
        "    model = PeftModel.from_pretrained(model, adapters_name)\n",
        "    model.eval()\n",
        "\n",
        "    tokenizer = transformers.AutoTokenizer.from_pretrained(model_identifier)\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "def predict_fn(question, model_and_tokenizer):\n",
        "    model, tokenizer = model_and_tokenizer\n",
        "\n",
        "    prompt = f\"Finance Question: {question}\\nAnswer:\"\n",
        "\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            max_length=100,  # Adjust the max length as needed\n",
        "            do_sample=True,\n",
        "            top_k=50,\n",
        "            temperature=1.0,\n",
        "            top_p=1.0,\n",
        "            repetition_penalty=1.1,\n",
        "        )\n",
        "        response = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
        "\n",
        "    return response\n",
        "\n",
        "# Set your finance model identifier and input finance question here\n",
        "finance_model_and_tokenizer = model_fn(finance_model_identifier)\n",
        "input_finance_question = \"What is a stock market?\"\n",
        "\n",
        "# Generate answer to the finance question\n",
        "finance_answer = predict_fn(input_finance_question, finance_model_and_tokenizer)\n",
        "\n",
        "# Print the generated finance answer\n",
        "print(\"Generated finance answer:\", finance_answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoMlPNng1z0J",
        "outputId": "d8a17424-dda2-49a5-8a54-10b6c2467778"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated finance answer: Finance Question: What is a stock market?\n",
            "Answer: \"This is this place to be! If you put on your own.\" \n",
            "Mummy said yes. She decided to go outside and play. As it went away, she was glad he had always made. She had such something special for her that was not a better thing to do.\n",
            "Soon, the most exciting job of her friends who came by they stopped her. \n",
            "Maya would have a great\n"
          ]
        }
      ]
    }
  ]
}